{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17AkTfO9efpnWhrdt8w2zKapqp999JyWp",
      "authorship_tag": "ABX9TyNjCJYkBy3Gwt4ekHMstqFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuj0630/Catholic_Regacy/blob/main/style_transfer_tutorial_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC5PdxAEGQus"
      },
      "outputs": [],
      "source": [
        "# 필요한 PyTorch 라이브러리 불러오기\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU 장치 사용 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2-JRG6IAGVly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_loader(img_path, imsize):\n",
        "  loader = transforms.Compose([\n",
        "      transforms.Resize(imsize), #이미지의 크기를 변경\n",
        "      transforms.ToTensor() # torch, Tensor 형식으로 변경\n",
        "  ])\n",
        "  image = PIL.Image.open(img_path) \n",
        "  # 네트워크 입력에 들어갈 이미지에 배치 목적의 차원 추가\n",
        "  image = loader(image).unsqueeze(0)\n",
        "  return image.to(device, torch.float) # GPU로 올리기\n",
        "\n",
        "# torch, Tensor 형태의 이미지를 화면에 출력\n",
        "def imshow(tensor):\n",
        "  #matplotlib는 CPU 기반이므로 CPU로 옮기기\n",
        "  image = tensor.cpu().clone()\n",
        "  # torch.Tensor에서 사용되는 배치 목적의 차원 제거\n",
        "  image = image.squeeze(0)\n",
        "  image = transforms.ToPIL()(image)\n",
        "  # 이미지를 화면에 출력(matplotlib는 [0, 1] 사이의 값이라고 해도 정상적으로 처라)\n",
        "  plt.imshow(image)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "GkjieFaRG5gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ndb796/Deep_Learning-Paper-Review-and-Practice\n",
        "%cd Deep-Learning-Paper-Review-and-Practice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbMQw08qIkmj",
        "outputId": "914af325-f526-4136-bca0-0a89d48df3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Deep_Learning-Paper-Review-and-Practice'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "[Errno 2] No such file or directory: 'Deep-Learning-Paper-Review-and-Practice'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Reconstruction 실습"
      ],
      "metadata": {
        "id": "c-29V7OJI0bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imag_path = './code_practice/images/cat.jpg'\n",
        "target_image = image_loader(img_path, (512, 512))\n",
        "imshow(target_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "t28O_wW5IynW",
        "outputId": "fd1d0b25-9635-4314-8a53-e0a4302ed961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4940da6d55d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimag_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./code_practice/images/cat.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtarget_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 동일한 크기의 노이즈 이미지 준비하기\n",
        "noise = torch.empty_like(target_image).uniform_(0, 1).to(device)\n",
        "imshow(noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "uc0zBEJrJFBV",
        "outputId": "ba409535-28f3-49eb-8b1f-057ca6d9913f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-09a27e79e748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 동일한 크기의 노이즈 이미지 준비하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.MSELoss() # 손실(loss) 함수 설정\n",
        "iters = 100 #반복 횟수 설정\n",
        "lr = 1e4\n",
        "\n",
        "print(\"[Start]\")\n",
        "imshow(noise)\n",
        "\n",
        "for i in range(iters):\n",
        "  # required_grad 속성의 값을 True로 설정하여 해당 torch, Tensor의 연산을 추적\n",
        "  noise.requires_grad = True\n",
        "\n",
        "  #손실 함수에 대하여 미분하여 기울기 계산\n",
        "  output = loss(noise, target_image)\n",
        "  output.backward()\n",
        "\n",
        "  #계산된 기울기를 이용하여 손실함수가 감소하는 방향으로 업데이트\n",
        "  gradient = lr * noise.grad\n",
        "  # 결과적으로 노이즈의 각 픽셀의 값이 [-eps, eps] 사이의 값이 되도록 자르기\n",
        "  noise = torch.clamp(noise = gradient, min=0, max=1).detach_()\n",
        "\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print(f'[Step: {i + 1}]')\n",
        "    print(f\"Loss: {output}\")\n",
        "    imshow(noise)"
      ],
      "metadata": {
        "id": "4fCVlcrwMiHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습을 위한 이미지 불러오기"
      ],
      "metadata": {
        "id": "CmPAsvGJNwB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 콘텐츠 이미지와 스타일 이미지를 모두 준비합니다.\n",
        "content_img = image_loader('./code_practices/images/content_img_1.jpg', (512, 640))\n",
        "style_img = image_loader('./code_practices/images/style_img_1.jpg', (512, 640))\n",
        "\n",
        "print(\"[Content Image]\")\n",
        "imshow(content_img)\n",
        "print(\"[Style Image]\")\n",
        "imshow(style_img)"
      ],
      "metadata": {
        "id": "AG2CTuYNKe4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN 네트워크 불러오기"
      ],
      "metadata": {
        "id": "hBsQEPyCOREf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴럴 네트워크 모델을 불러옵니다.\n",
        "cnn = models.vgg19(pretained=True),features.to(device).eval()\n",
        "print(cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "7hdenJS6OQFh",
        "outputId": "d862d45a-f731-4532-dabf-dbb7f6229fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fb41810830f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 뉴럴 네트워크 모델을 불러옵니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_only_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36minner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights_param\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_weights_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mvgg19\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"E\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36m_vgg\u001b[0;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0m_ovewrite_named_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_classes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'pretained'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 정규화를 위한 초기화\n",
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(Normalization, self).__init__()\n",
        "    self.mean = mean.clone().view(-1, 1, 1)\n",
        "    self.std = std.clone().view(-1, 1, 1)\n",
        "\n",
        "  def forward(self, img):\n",
        "    return (img - self.mean) / self.std  "
      ],
      "metadata": {
        "id": "ohBHfb4wOhk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Style Reconstruction 실습하기"
      ],
      "metadata": {
        "id": "H0Qa00stPt2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   임의의 노이즈가 측정한 이미지의 스타일을 가지도록 변환합니다.\n",
        "*   style_layers 리스트 변수의 값을 조절하여 어떤 레이어를 이용할지 설정할 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "0lNkTgWNP0I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(input):\n",
        "  a, b, c, d = input.size()\n",
        "  features = input.view(a * b, c * d)\n",
        "  G = torch.mm(features, features.t())\n",
        "  return G.div(a * b * c * d)\n",
        "\n",
        "# 스타일 손실 계산을 위한 클래스 정의\n",
        "class StyleLoss(nn.Module):\n",
        "  def __init__(self, target_feature):\n",
        "    super(StyleLoss, self).__init__()\n",
        "    self.target = gram_matrix(target_feature).detach()\n",
        "\n",
        "  def forward(self, input):\n",
        "    G = gram_matrix(input)\n",
        "    self.loss = F.mse_loss(G, self.target)\n",
        "    return input     "
      ],
      "metadata": {
        "id": "-b2-ulcRPsyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "# 스타일 손실을 계산하는 함수\n",
        "def get_style_losses(cnn, style_img, noise_image"
      ],
      "metadata": {
        "id": "oJrWRthTQ2bH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}